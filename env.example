# This file serves as a template for the .env file.
# Copy this file to .env and fill in your actual values.
# Lines starting with # are comments.
 
# === General Project Configuration ===
# Path to the input data file (Excel or CSV). Relative to the project root.
INPUT_EXCEL_FILE_PATH="data/Apollo Shopware Partner Germany 1(Sheet1).csv"
 
# Specifies a range of rows (1-based inclusive) or a number of rows to process from the input file.
# Examples: "10-20" (rows 10-20), "20" (first 20), "10-" (row 10 to end), "-20" (first 20), "" or "0" (all rows).
ROW_PROCESSING_RANGE=""
 
# Base directory for all output files. Relative to the project root. Will be created if it doesn't exist.
OUTPUT_BASE_DIR="output_data"
 
# Template for the main summary Excel report file name. {run_id} will be replaced.
OUTPUT_EXCEL_FILE_NAME_TEMPLATE="Pipeline_Summary_Report_{run_id}.xlsx"
 
# Specifies which input column mapping profile to use from AppConfig.INPUT_COLUMN_PROFILES.
# See src/core/config.py for profile definitions (e.g., "default", "lean_formatted", "ManauvKlaus").
INPUT_FILE_PROFILE_NAME="apollo_shopware_partner_germany"
 
# Number of consecutive empty rows to detect as end-of-data when ROW_PROCESSING_RANGE is open-ended.
CONSECUTIVE_EMPTY_ROWS_TO_STOP="3"
 
# === Filename Configuration for Output Files ===
# Max length for the sanitized company name part of output filenames.
FILENAME_COMPANY_NAME_MAX_LEN="25"
# Max length for the sanitized URL domain part of output filenames.
FILENAME_URL_DOMAIN_MAX_LEN="8"
# Max length for the URL hash part of output filenames.
FILENAME_URL_HASH_MAX_LEN="8"
 
# === Logging Configuration ===
# Log level for the main log file (e.g., DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL="INFO"
# Log level for the console output (e.g., DEBUG, INFO, WARNING, ERROR)
CONSOLE_LOG_LEVEL="WARNING"
 
# === LLM Configuration (Google Gemini) ===
# REQUIRED: Your API key for the Google Gemini service.
GEMINI_API_KEY=""
 
# Gemini model to use (e.g., "gemini-1.5-pro-latest", "gemini-1.5-flash-latest").
LLM_MODEL_NAME="gemini-2.0-flash"
LLM_MODEL_NAME_SALES_INSIGHTS="gemini-2.5-pro-preview-06-05"
# Temperature for general-purpose LLM calls.
LLM_TEMPERATURE_DEFAULT="0.3"
 
# Temperature for precise data extraction tasks (lower is better).
LLM_TEMPERATURE_EXTRACTION="0.2"
 
# Temperature for creative tasks like sales pitch generation (higher allows more creativity).
LLM_TEMPERATURE_CREATIVE="0.5"
 
# Optional: Specific temperature for summarization tasks. If blank, uses LLM_TEMPERATURE_DEFAULT.
LLM_TEMPERATURE_SUMMARY=""
 
LLM_MAX_TOKENS="3000"
LLM_TOP_K=""
LLM_TOP_P=""
 
# === Extraction Profiles and Prompt Paths (relative to project root) ===
# Active extraction profile: "minimal", "minimal_plus_summary", "enriched_direct" (future).
EXTRACTION_PROFILE="minimal"
 
# Prompt for generating homepage context (company name, summary, industry).
# This is primarily used by the phone-retrieval pipeline as extra context for the LLM.
# If you want German artifacts/outputs, prefer the *_de.txt variants.
PROMPT_PATH_HOMEPAGE_CONTEXT="prompts/homepage_context_prompt.txt"
 
# Prompt for general summarization tasks (if used separately).
PROMPT_PATH_SUMMARIZATION="prompts/summarization_prompt.txt"
 
# Prompt for website text summarization.
# If you want German summaries in outputs/reports, use:
#   prompts/website_summarizer_prompt_de.txt
PROMPT_PATH_WEBSITE_SUMMARIZER="prompts/website_summarizer_prompt_de.txt"
 
# Maximum characters of website text to feed into the summarization LLM.
LLM_MAX_INPUT_CHARS_FOR_SUMMARY="40000"
 
# Number of top-priority pages the scraper should collect text from for summarization.
SCRAPER_PAGES_FOR_SUMMARY_COUNT="3"
# === Web Scraper Configuration ===
# A comma-separated list of User-Agent strings to be rotated for scraper requests.
SCRAPER_USER_AGENTS="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36,Mozilla/5.0 (Macintosh; Intel Mac OS X 14_7_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36,Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36"
 
# A JSON string of default headers to be sent with every scraper request.
SCRAPER_DEFAULT_HEADERS='{"Accept-Language": "en-US,en;q=0.9", "Accept-Encoding": "gzip, deflate, br", "Connection": "keep-alive", "Referer": "https://www.google.com/"}'
SCRAPER_PAGE_TIMEOUT_MS="45000" #15000
SCRAPER_NAVIGATION_TIMEOUT_MS="45000" #15000
SCRAPER_MAX_RETRIES="1"
SCRAPER_RETRY_DELAY_SECONDS="2"
SCRAPER_NETWORKIDLE_TIMEOUT_MS="4000" # Default 3s, 0 to disable. #1500
MAX_DEPTH_INTERNAL_LINKS="1"
 
# Keywords to identify relevant internal links. Comma-separated.
TARGET_LINK_KEYWORDS="about,company,services,products,solutions,team,mission"
# Keywords for top-priority pages (e.g., "Impressum", "Kontakt"). Comma-separated.
SCRAPER_CRITICAL_PRIORITY_KEYWORDS="about-us,company-profile"
# Keywords for high-priority pages (e.g., "Legal", "Privacy"). Comma-separated.
SCRAPER_HIGH_PRIORITY_KEYWORDS="services,products,solutions"
# Max path segments for a priority keyword to retain its highest score tier.
SCRAPER_MAX_KEYWORD_PATH_SEGMENTS="3"
# URL path patterns to hard-exclude from scraping. Comma-separated.
SCRAPER_EXCLUDE_LINK_PATH_PATTERNS="/media/,/blog/,/wp-content/,/video/,/hilfe-video/"
# Max pages to scrape per domain (0 for no limit).
SCRAPER_MAX_PAGES_PER_DOMAIN="5"
# Minimum score a link needs to be added to the scrape queue.
SCRAPER_MIN_SCORE_TO_QUEUE="40"
# Score threshold for a page to bypass SCRAPER_MAX_PAGES_PER_DOMAIN.
SCRAPER_SCORE_THRESHOLD_FOR_LIMIT_BYPASS="80"
# Max additional high-priority pages to scrape after SCRAPER_MAX_PAGES_PER_DOMAIN is hit.
SCRAPER_MAX_HIGH_PRIORITY_PAGES_AFTER_LIMIT="2"
 
# Whether the scraper should respect robots.txt (True/False).
RESPECT_ROBOTS_TXT="True"
# User-agent string for checking robots.txt.
ROBOTS_TXT_USER_AGENT="*"
 
# === URL Handling ===
# TLDs to try appending to domain-like inputs lacking a TLD. Comma-separated.
URL_PROBING_TLDS="de,com,at,ch"
# Enable DNS error fallback strategies (True/False).
ENABLE_DNS_ERROR_FALLBACKS="True"
 
#
# Attribute extraction prompt (industry, products/services, USPs, target segments, etc.)
# If you want these fields in German, use:
#   prompts/attribute_extractor_prompt_de.txt
PROMPT_PATH_ATTRIBUTE_EXTRACTOR="prompts/attribute_extractor_prompt_de.txt"
# Path to the Golden Partners CSV file
PATH_TO_GOLDEN_PARTNERS_DATA="data/kgs_001_ER47_20250626.xlsx"
# Maximum number of Golden Partner summaries to include in the prompt for LLM Call 3.
MAX_GOLDEN_PARTNERS_IN_PROMPT="50"
SALES_PROMPT_LANGUAGE="de"
 
# === German Language Prompt Paths (relative to project root) ===
# These are optional. If left blank, the default paths from the AppConfig class will be used.
PROMPT_PATH_GERMAN_PARTNER_MATCHING="prompts/german_partner_matching_prompt.txt"
PROMPT_PATH_GERMAN_SALES_PITCH_GENERATION="prompts/german_sales_pitch_generation_prompt.txt"
# === Advanced Scraper Features ===
PROXY_ENABLED=False
CACHING_ENABLED=False
INTERACTION_HANDLER_ENABLED=True
# === Caching Configuration ===
CACHE_DIR="cache"
 
# === Proxy Configuration ===
# Comma-separated list of proxies, e.g., "http://user:pass@host:port,http://user2:pass2@host2:port2"
PROXY_LIST=""
PROXY_ROTATION_STRATEGY="random" # 'random' or 'sequential'
PROXY_HEALTH_CHECK_ENABLED="True"
PROXY_COOLDOWN_SECONDS="300"
 
# === Interaction Handler Configuration ===
# Comma-separated CSS selectors for cookie banners, etc.
INTERACTION_SELECTORS="button[id*=\"accept\"],button[id*=\"agree\"],button[id*=\"consent\"],button[id*=\"cookie\"]"
# Comma-separated text queries for cookie banners, etc.
INTERACTION_TEXT_QUERIES="accept,allow,agree,consent,confirm,continue,ok,got it,close,accept all,allow all,i agree,i accept,i consent,yes, i agree,accept cookies,allow cookies,accept all cookies,allow all cookies,save and exit,save settings,save preferences,accept & close,alle cookies akzeptieren,cookies zulassen,ich stimme zu,zustimmen,einverstanden,ja,akzeptieren,alles akzeptieren,alle akzeptieren,alle cookies erlauben,akzeptieren und fortfahren,speichern & akzeptieren,cookies aktivieren,alle annehmen,akzeptiere,erlauben,verstanden,bestätigen,weiter,fortsetzen,schließen,cookies akzeptieren,cookies erlauben,alle cookies annehmen,ich akzeptiere,ja, ich stimme zu,speichern und schließen,einstellungen speichern,präferenzen speichern,akzeptieren & schließen"
INTERACTION_HANDLER_TIMEOUT_SECONDS="5"
 
# === Slack Notification Settings ===
# Set to "True" to enable Slack notifications upon pipeline completion.
ENABLE_SLACK_NOTIFICATIONS="True"
# Your Slack Bot User OAuth Token (starts with "xoxb-").
SLACK_BOT_TOKEN=""
# The ID of the Slack channel to send notifications to (e.g., "C1234567890").
SLACK_CHANNEL_ID=""
 
# Optional suffix to append to the run ID for easier identification (e.g., "batch1", "test_run")
RUN_ID_SUFFIX=


# SCRAPER_HTTP_FALLBACK_ENABLED (default True)
# SCRAPER_HTTP_FALLBACK_TIMEOUT_SECONDS (default 15)
# SCRAPER_HTTP_FALLBACK_MAX_BYTES (default 2000000)

AUGMENTED_PHONE_TEXT_PREFIX="'"